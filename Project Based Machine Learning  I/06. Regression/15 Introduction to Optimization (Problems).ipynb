{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5ee8ef-f2ba-46f5-a304-2622e41cecaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <img src=\"../../images/logos/ml-logo.png\" width=\"23\"/> Introduction to Optimization (Problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4421d4-6f9d-4926-ac88-6e2f7ba271af",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Which of the following concepts is represented by the statement \"It is an optimization algorithm and the goal of it is to find the values of the model parameters that minimize the cost function.\" in machine learning?\n",
    "\n",
    "1. Regularization\n",
    "2. Cross-validation\n",
    "3. Gradient descent\n",
    "4. Early stopping\n",
    "\n",
    "**Answer:**  3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e96c04a-5265-41df-b7c1-e7cc395c2278",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ad7cf-f3d9-4d04-acc1-3c3dd5805620",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Which of the following can complete the following statement?  \n",
    "\"Gradient descent enables us to find...\"\n",
    "1. the stationary points where the gradient is zero.\n",
    "2. the global minimum of the loss function.\n",
    "3. the local minimum of the loss function.\n",
    "4. the maximum value of the loss function.\n",
    "\n",
    "**Answer:** 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6c934-2004-4f0e-a9ec-c1629cc43d50",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de4ad4-d7e9-46df-913a-a2131149b0b0",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Which of the following is true about the **cost function** in linear regression and optimizing it using gradient descent?\n",
    "\n",
    "1. It is discontinuous, but it may not be convex, gradient descent may converge to a local minimum.\n",
    "4. It is discontinuous and convex, gradient descent is guaranteed to converge to the global minimum.\n",
    "2. It is continuous, but it may not be convex and gradient descent may converge to a local minimum.\n",
    "3. It is continuous and convex, gradient descent is guaranteed to converge to the global minimum.\n",
    "\n",
    "**Answer:**  4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e6dd05-8cfc-4e61-aa4c-1870282321e8",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
