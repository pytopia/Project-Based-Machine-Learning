{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5ee8ef-f2ba-46f5-a304-2622e41cecaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <img src=\"../../images/logos/ml-logo.png\" width=\"23\"/> Mini-Batch Gradient Descent (Problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfdeae7-c12e-4893-863b-f2f8d7bd4386",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Which of the following statements describes **mini-batch gradient descent** (MGD)?\n",
    "\n",
    "1. It uses the entire training set to compute the gradient of the loss function.\n",
    "2. It uses a single training example to compute the gradient of the loss function.\n",
    "3. It splits the training datasets into small batches to computes the gradient of the loss function in each iteration and update model coefficients.\n",
    "4. It computes the gradient of the loss function using a random subset of the training set in each iteration.\n",
    "\n",
    "**Answer:**  3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad7304-b847-4889-a586-b85502e3da74",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36564f5-39b5-4855-afaa-fb0fc1049bc8",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "What does the **number of epochs** show in mini-batch gradient descent?\n",
    "\n",
    "1. The number of iterations over the entire training dataset.\n",
    "2. The size of the mini-batch used to train the model.\n",
    "3. The learning rate used to update the model parameters.\n",
    "4. The number of times the entire training dataset is used to update the model parameters.\n",
    "\n",
    "**Answer:**  4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87ab1d-a0d0-4c58-888d-caf4d3120023",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d3b76-14a3-4c53-8453-726921af7b2d",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "What is the difference between **batch** and **epoch** in mini-batch gradient descent?\n",
    "\n",
    "1. Batch refers to the entire training dataset, while epoch refers to a subset of the training dataset.\n",
    "2. Batch refers to a subset of the training dataset, while epoch refers to the entire training dataset.\n",
    "3. Batch and epoch both refer to the entire training dataset, but they are used interchangeably.\n",
    "4. Batch and epoch both refer to a subset of the training dataset, but they are used interchangeably.\n",
    "\n",
    "**Answer:**  2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878d04b-cc9e-47b0-bade-15366c40ea99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a657f1e-4a6c-4318-84d0-824cd4d1b5d2",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Which of the following is a useful NumPy function to help create shuffled indices for creating mini-batches in mini-batch gradient descent?\n",
    "\n",
    "1. np.random.permutation()\n",
    "2. np.random.normal()\n",
    "3. np.random.uniform()\n",
    "4. np.random.choice()\n",
    "\n",
    "**Answer:**  1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd934014-548b-48e0-8c81-f3aab9d1ea6a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8cc4f-bdab-47f1-a163-58ec75cc4e8f",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Which of the following are **advantages** of using mini-batch gradient descent instead of using batch gradient descent?\n",
    "\n",
    "1. It reduces the computational cost by processing the data in smaller subsets.\n",
    "2. It allows for better generalization by introducing more stochasticity in the gradient estimation.\n",
    "3. It can converge faster by making more frequent updates to the model parameters.\n",
    "4. All of the above.\n",
    "\n",
    "**Answer:**  4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3191baf-d6b4-4526-9600-d79705c13f87",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2a3d4-4bb9-4dfe-a850-c9008b800119",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Which of the following are **disadvantages** of using mini-batch gradient descent instead of using batch gradient descent?\n",
    "\n",
    "1. It can be computationally expensive for large datasets.\n",
    "2. It can lead to slower convergence due to the added stochasticity in the gradient estimation.\n",
    "3. It can be more prone to overfitting compared to batch gradient descent.\n",
    "4. Both 2 and 3\n",
    "\n",
    "**Answer:**  4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d0bc0-e531-48b6-8aaf-1882f4783c0d",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
