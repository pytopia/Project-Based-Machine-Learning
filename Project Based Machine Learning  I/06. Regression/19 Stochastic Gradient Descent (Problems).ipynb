{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5ee8ef-f2ba-46f5-a304-2622e41cecaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <img src=\"../../images/logos/ml-logo.png\" width=\"23\"/> Stochastic Gradient Descent (Problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19422f19-f337-4ff1-99a9-02071f6d09db",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "Which of the following can complete the sentence below? \n",
    "\n",
    "\"Stochastic gradient descent (SGD) is an optimization algorithm that computes the gradient of the loss function using...\"\n",
    "1. the entire training dataset.\n",
    "2. a fixed subset of the training dataset.\n",
    "3. a single random instance of training dataset at a time.\n",
    "4. an adaptive learning rate.\n",
    "\n",
    "**Answer:**  3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e6652-31ab-4faf-b9d8-26ba187ec0de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301df8f-b449-4c75-9bd7-a2cd72f43450",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "What is the **advantage** of stochastic gradient descent?\n",
    "\n",
    "1. SGD can converge faster than batch gradient descent by making more frequent updates to the model parameters.\n",
    "2. SGD can handle larger datasets than batch gradient descent by processing the data in smaller subsets.\n",
    "3. SGD can converge to a better solution than batch gradient descent by escaping from local minima.\n",
    "4. All of the above.\n",
    "\n",
    "**Answer:**  4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58314230-fa8b-45a4-a137-c2edb34ebc07",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f86e90-a54f-47e9-92cb-eb893c91fcc2",
   "metadata": {},
   "source": [
    "**Question:**  \n",
    "What is the **disadvantage** of stochastic gradient descent?\n",
    "\n",
    "1. The updates in SGD are noisy and have a high variance, which can make the optimization process less stable and lead to oscillations around the minimum.\n",
    "2. It may require more iterations to converge to the minimum since it updates the parameters for each training example one at a time.\n",
    "3. It can be sensitive to the learning rate.\n",
    "4. All of the above.\n",
    "\n",
    "**Answer:**  4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e9433-a3c2-4e85-a0bd-41264422ea8b",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
